
RoDynRF： Robust Dynamic Radiance Fields

![系统架构](assets/doctor/RoDynRF_fig2.png){width=100%}

1. 同时重建静态场景nerf和动态场景nerf
1. 不要求相机位姿和相机内参
1. 网络设计和损失设计改进位姿估计和动态nerf重建
1. 在一些困难数据集上也可以正常估计相机位姿，而传统的sfm会失败.

## Method

### 静态重建和相机位姿估计

![Loss](assets/doctor/RoDynRF_fig3.png){width=100%}

1. 场景划分。分为静态区域和动态区域，使用不同的**显示**神经体素网格表示，$\mathrm{V}^s$ 与 $\mathrm{V}^s$ 。
1. 动态区域分割。利用Mask R-CNN和光流法得到的极线距离阈值约束，得到运动物体的掩码。
1. coarse-to-fine静态场景重建。同时优化输入帧的相机位姿和共享的焦距，使用coarse-to-fine的策略优化静态场景的表示。
1. 末期视角方向条件。在color mlp的最后一层才混合视角方向。优化时绕过神经体素网格，直接学习视角方向到输出采样颜色的映射函数。
1. 损失函数。光度损失
    1. 重投影损失。像素体渲染得到3d点，投影，RAFT寻找对应关系。
    1. 视差损失。对应点体渲染得到3d点对，计算z差
    1. 单目深度损失。MiDaSv2.1估计出未知尺度和平移的深度图，约束渲染的深度。
    1. 优化方法。模拟退火算法，光流估计和深度图估计可能不准确，只在训练的初期参与指导。

### 动态nerf

1. 光度损失
1. 场景流损失。为了得到3d点的运动，引入了场景流mlp来补偿3d运动。
    1. 场景流mlp预测的点和形变mlp预测的点的重投影误差、视差、单目深度损失。
    1. 正则化损失。
    1. mask损失。
1. 线性组合。将静态损失和动态损失线性组合起来。
