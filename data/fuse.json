{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Pub","n":1}}},{"i":2,"$":{"0":{"v":"Doctor","n":1}}},{"i":3,"$":{"0":{"v":"Writing","n":1},"1":{"v":"\n# En\n\n## Example\n\n> We reduce this cost with a **versatile** new input encoding that permits the use of a smaller network without **sacrificing** quality, thus significantly reducing the number of floating point and memory access operations. \n> - 多才多艺的. \n\n> This approach tends to yield a larger degree of adaptivity compared with the previous parametric encodings, **albeit** at greater computational cost which can only be amortized when sufficiently many inputs x fall into each leaf node.\n> - Although.\n\n> which have **on the order of** 10 000 entries. \n> - About.\n\n> Without interpolation, **grid-aligned discontinuities** would be present in the network output, which would result in an **undesirable blocky appearance**\n> - 网格不连续，会有空洞\n\n\n\n\n## Logic\n\n\n\n\n# Zh\n\n","n":0.094}}},{"i":4,"$":{"0":{"v":"SLAM","n":1},"1":{"v":"\n<style>\n.align-right {\n  float: right;\n}\n</style>\n\n> <abbr title=\"Simultaneous Localization and Mapping\">SLAM</abbr> stands for Simultaneous Localization and Mapping. It is a computational technique used in robotics and computer vision to enable an autonomous system, such as a robot or a self-driving car, to build a map of its environment while simultaneously determining its own location within that map. (come from chatGPT) <div style=\"text-align: right\"> (come frome chatGPT) </div>\n\n","n":0.124}}},{"i":5,"$":{"0":{"v":"(TRO 2018)VINS-mono","n":0.707},"1":{"v":"\nVINS-Mono: A Robust and Versatile Monocular Visual-Inertial State EstimatorS\n\n## Overview\n\n相比于OKVIS：\n- 单目。OKVIS是多目\n- 单目初始化、关键帧选择策略、大视角相机\n- 闭环检测、位姿图重使用\n\n### Measurement Preprocessing\n\n*A. Vision Processing Front End*\n\n- 特征点追踪。使用KLT稀疏光流法追踪特征点\n- 角点检测。每张图最大（100-300）特征点\n- 外点拒绝。基础矩阵RANSAC模型\n\n关键帧选择\n\n- 视差。计算与前一帧的平均视差。平移和旋转都会影响视差。视差越大，越有可能是关键帧。只有旋转时，不能通过三角化计算视差，此时使用陀螺仪短期积分替代，此积分只用在关键帧选择中。\n- 追踪质量。追踪特征下降到一个阈值，则该帧为关键帧。\n\n*B. IMU Preintegration*\n\n### Estimator Initialization\n\n*A. Vision-Only SfM in Sliding Window*\n\n*B. Visual-Inertial Alignment*\n\n1. Gyroscope Bias Calibration\n1. Velocity, Gravity Vector, and Metric Scale Initialization\n1. Gravity Refinement.\n1. Completing Initialization\n\n- 视觉残差的自由度是2\n\n","n":0.141}}},{"i":6,"$":{"0":{"v":"(ICRA 2021)Towards Real-time Semantic RGB-D SLAM in Dynamic Environments","n":0.333},"1":{"v":"\n> However for consistent long-term map building this is not necessary as these movable objects are undesired even when they are temporarily static\n\n通过语义把潜在的动态物体过滤掉，本来应该是缺点，但是换一个角度，也可以说是优点，有预测能力，有助于建立长期一致的地图。\n\n![系统](assets/doctor/Towards%20Real-time%20Semantic%20RGB-D%20SLAM%20in%20Dynamic%20Environments_fig2.png)\n\n- 使用segnet网络做语义分割，训练集为PASCAL VOC，使用单独的线程。\n- 使用K-Means对depth聚类，计算每个簇的重投影误差。$r_j=\\frac{1}{m}\\sum_{i \\in c_j}^{m} \\rho(||\\mathbf{u}_i'-\\pi(\\mathbf(T_{wc}\\mathbf{P_i}||^2)$ ，$P_i$ 和 $\\mathbf{u}_i$是orb特征匹配的点\n- 关键帧策略和orbslam2相同，对关键帧做语义分割。\n- 执行两阶段追踪。使用关键帧做第一阶段追踪找动态点，使用local map做第二阶段追踪。\n","n":0.169}}},{"i":7,"$":{"0":{"v":"SobolevFusio","n":1}}},{"i":8,"$":{"0":{"v":"(arXiv 2023)RGB-D-Inertial SLAM in Indoor Dynamic Environments with Long-term LargeOcclusion","n":0.316}}},{"i":9,"$":{"0":{"v":"(ICRA 2022)Flow Supervised Neural Radiance Fields for Static-Dynamic Decomposition","n":0.333}}},{"i":10,"$":{"0":{"v":"(ICRA 2021)DroidSLAM","n":0.707}}},{"i":11,"$":{"0":{"v":"(arXiv 2023)Deflow","n":0.707}}},{"i":12,"$":{"0":{"v":"Review","n":1},"1":{"v":"\n# 论文整理\n\n## 传统方法\n\n|论文 | 解决的问题 | 采用的方法 | 评价 | \n|--- | --- | --- | --- |\n|Refusion (IROS 2019)|剔除运动物体后的三维重建系统|使用残差找到动态部分的点，使用填充算法得到动态部分区域，剔除动态部分使用静态部分重建|分割结果总是比实际的要大、重建失败、重影|\n|BAD Slam (CVPR 2019)| 实时重建 | 基于Surfel的BA快速rgb-d BA | \n|Gradient-SDF (CVPR 2022)| 静态重建，提出了新的三维模型表示方法，同时结合了显式（BA)和隐式(SDF Tracking)的优点 | voxel存储存储梯度，可以计算出voxel对应的表面上最近的点，非常容易做相机追踪和BA，提高重建模型的精度| \n|HRBF-Fusion(Siggraph 2022)|静态高精度三维重建|提出了一种新的基于Surfel的模型表示方法-Hermit Radial Basis Function，用于改进局部细节。并提出了基于HRBF的曲率、法线、置信度和新的融合策略，构建了3D重建系统|尝试使用了新的模型表示方法，不太容易改进|\n|DRG-SLAM（IROS 2022）|删除运动物体的稀疏建模 | 动态点不满足极线约束、使用网络分割和多视图一致性去掉动态orb特征点，使用点线面损失计算相机位姿 | 可以去掉分割网络不能识别的动态物体|\n|ACEFusion（IROS 2022）|同时重建前景和背景，rgbd实时动态环境重建|利用深度学习加速单元加快实例分割，使用光流和实例分割的方法得到初始的动态区域，然后使用flood-fill得到最终的动态区域掩码，之后使用octree重建静态场景，使用sufelwarp重建动态区域|同时重建动态区域和静态区域|\n|PSgraident-SDF (WACV 2023) | 光度立体重建：多视角RGB-D高精度重建系统，需要重建几何细节和丰富的纹理 | 给定模型的BRDF、法线和光照模型的初始值，利用渲染后的结果和拍摄得到的RGB图像来优化模型的BRDF、符号距离值以及相机位姿和灯光| \n\n\n---\n## 深度学习\n\n| 论文 | 解决的问题 | 采用的方法 | 评价 |\n|--- | --- | --- | --- |\n|Deepdeform(2019)|非刚性重建|神经网络来找前后像素的对应关系|\n|Neural non-rigid tracking (CVPR 2020)|非刚性重建|引入了稠密光流|比deepdeform更高效|\n|4DComplete (ICCV 2021)|非刚性重建|预测不可见表面的运动|计算复杂，不能实时|\n|iMAP (ICCV 2021)|实时静态重建|使用隐式场表示三维模型，利用隐式场Nerf得到的深度和颜色与输入的损失来更新相机的位姿和网络。并提出了相应的关键帧选择策略|\n|OcclusionFusion（CVPR 2022)|非刚性重建,相机固定| 用光流估计2D运动，从而得到可见部分的三维运动，图神经网络和LSTM综合考虑了历史运动、当前运动信息来预测当前帧运动和置信度。完成t+1帧重建|基于图节点重建系统无法解决拓扑该改变(POSEFusion或者Neural Deformation Graphs）、长期遮挡的部分和大的形变|\n|Neural RGB-D Surface Reconstruction（CVPR 2022）| 高精度静态重建|执行BundleFusion得到初始的相机位姿估计，使用不同的mlp分别计算sdf和color，然后神经渲染得到的深度图和颜色图。|\n|BNV-Fusion (CVPR 2022)|高精度静态重建| 使用神经网络编码每一个深度图，然后bi-level融合成全局模型，然后优化全局模型 |速度比传统方法慢、深度图s测量噪声会造成几何不连续|\n|Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera（NIPS 2022）|RGBD动态场景重建，相机固定|使用可逆双射函数来预测规范空间中点的形变之后的位置，然后结合拓扑感知网络可以得到mlp sdf，通过神经渲染函数，可以得到该点的颜色，通过在视线方向上做积分，可以得到该视角下渲染后的结果。| - 效果依旧不如orb-slam2，可能是BA，无全局ba，预测能力取决于coarse fine|\n|vox-fusion(ISMAR 2022)|高精度静态重建|相比于imap，网络模型表示不一样，八叉树+体素编码+全连接。可以表示free space，关键帧选择策略选择新出现区域多的帧。|","n":0.097}}},{"i":13,"$":{"0":{"v":"Nerf","n":1},"1":{"v":"\n> <abbr title=\"Neural Radiance Fields\">NeRF</abbr> is a technique used in computer graphics and computer vision that aims to generate realistic 3D scenes from 2D images or a limited set of 2D views. It is particularly effective in creating high-quality renderings of complex scenes with detailed lighting and geometry. <div style=\"text-align: right\"> (come frome chatGPT) </div>\n\n> 用体渲染组织神经图元，实现真实的新视角合成。\n\n>  the low-frequency information in a neural network tends to converge faster than the higher-frequency information.\n<div style=\"text-align: right\"> (Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS, 2020.) </div>\n\n","n":0.107}}},{"i":14,"$":{"0":{"v":"(CVPR 2023)vMap","n":0.707},"1":{"v":"\n做多个物体级别的实时重建任务。最多可以同时重建50个物体（MLP），5Hz的地图实时更新。\n\n![系统架构](assets/doctor/vMAP_fig2.png){width=100%}\n\n## 创新点\n\n1. 对场景的划分不同。DeRF将空间划分为较小的部分，然后使用小网络表示分解后的部分。KiloNeRF将空间划分为Volume,然后使用极小的MLP表示Volume元素，该方式可以加快nerf的训练。vMAP按物体级别的语义划分。\n1. **多物体** **实时**重建。\n\n\n## 方法\n\n**细节**\n1. RGBD物体掩码通过off-the-shelf分割网络预测，通过语义和空间一致性关联。\n1. 相机Tracking通过off-the-shelf追踪系统得到，比同时优化相机和几何效果好。\n1. 每一个物体都有自己的关键帧队列，不同物体之间的训练不会受到影响。\n1. 所有物体使用相同的网络，背景使用稍大的网络。网络可以stack\n之后，做向量化并行化训练。\n1. 光线采样。高斯分布采样+均匀采样。\n1. 网络输入忽略视角的影响。使用二值函数建模物体的可见性（假设没有透明物体）。\n\n**Loss**\n\n\n## 实现\n\n**设备** RTX 3090 GPU， 3.60 GHz i7-11700K CPU\n\n**框架选择**\n1. 相机位姿通过orb-slam3得到。关键帧位姿通过orb-slam3连续更新。\n1. 分割网络使用Detic。在LVIS上预训练，包含1000个物体类别。\n\n**超参数** \n","n":0.209}}},{"i":15,"$":{"0":{"v":"(RSS 2022)iSDF","n":0.707},"1":{"v":"\n使用rgbd实时直接预测sdf，而不是tsdf.\n\n","n":1}}},{"i":16,"$":{"0":{"v":"(CVPR 2021)iMAP","n":0.707},"1":{"v":"\n\n\n## 创新点\n\n1. 似是而非的补全水密重建","n":0.577}}},{"i":17,"$":{"0":{"v":"(ISMAR 2022)Vox-Fusion","n":0.707},"1":{"v":"\n[[(CVPR 2022)Nice-SLAM|pub.doctor.nerf.Nice-SLAM]]\n\n### 创新点 \n\n1. 使用三维模型的<span style=\"color:red\">混合</span>表达方式\n1. 使用八叉树来组织体素模型\n1. 基本实时，大概1-2 fps\n1. 使用了nerf强大的体渲染表达\n\n## 不足\n\n1. 不能处理动态物体，未使用BA\n\n## 方法\n### 网络结构\n\n![网络结构](assets/doctor/Vox-fusin_fig2.png){width=100%}\n\n### 渲染方程\n\n$$ \n\\begin{gather*} \n（\\mathbf{c}_i,s_i)=F_{\\theta}(\\mathrm{TriLerp}(\\hat{\\xi_i}T_i \\mathbf{p}_i,\\mathbf{e})), \\\\\nw_i=\\sigma(\\frac{s_i}{tr}) \\cdot \\sigma(-\\frac{s_i}{tr}), \\\\\n\\mathbf{C}=\\frac{1}{\\sum^{N-1}_{j=0} w_i} \\sum^{N-1}_{j=0}w_j c_j, \\\\\nD=\\frac{1}{\\sum^{N-1}_{j=0} w_i} \\sum_{j=0}^{N-1}w_j d_j\n\\end{gather*}\n$$\n\n### 损失函数\n\n$$\n\\begin{gather*} \n\\mathscr{L}_{rgb}=\\frac{1}{|P|}\\sum_{i=0}^{|P|}||\\mathbf{C_i}-\\mathbf{C}_i^{gt}|| \\\\\n\\mathscr{L}_{depth}=\\frac{1}{|P|}\\sum_{i=0}^{|P|}||D_i-D_i^{gt}||, \\\\\n\\mathscr{L}_{fs}=\\frac{1}{|P|}\\sum_{p \\in P} \\frac{1}{S_p^{fs}} \\sum_{s \\in S_p}^{fs}(D_s-tr)^2,\\\\\n\\mathscr{L}_{sdf}=\\frac{1}{|P|}\\sum_{p \\in P} \\frac{1}{S_p^{tr}} \\sum_{s \\in S_p^{tr}}(D_s-D_s^{gt})^2\n\\end{gather*}\n$$ \n\n## 改进优点\n\n1. 相比于Refusion, 可以得到无限分辨率的三维模型.多了BA模块,可以得到全局一致(去掉动态物体)的模型.\n1. 相比于先使用网络分割的方法，可以不依赖与任何先验知识，可以根据场景的运动，自动识别动态场景。\n1. 扩展了Vox-fusion的使用场景，使之nerf可以用到动态场景的三维重建中。\n\n假设是更相信当前的结果，通过当前的结果来优化过去的结果。\n\n## 改进算法\n\nKeyFrame队列中存储的应该是原始图片 $\\{I_k,D_k,M_k\\}$. 可以考虑给定第一帧的动态物体掩码。\n\n渲染网格状态：free space网格Vf、TSDF区域内的网格Vt，表面的网格Vs，未分配的网格Vu。\n\n动态物体网格状态变化：Vt->Vf, Vs->Vf. 如果是通过变化得到的Vf,那么不应该参与后续的优化。\n\n- Vf网格约束：多个视角光线下看到的该网格sdf是截断距离。\n- Vt网格约束：多个视角光线下看到的该网格的sdf是该角度下的符号距离。\n- Vs网格约束：多个视角光线下看到的该网格的sdf=0\n\n对于Vf的网络不做处理。只遍历Vt和Vs的网格.\n\n- 如果是静态网格,那么不同帧之间测到的同一个网格的sdf应该和gt-d是一致的。\n- 如果是动态网格,那么每一个网格中所有穿过的光线的sdf和gt-d的差值是不一致的。\n\n如果一个网格中不一致的光线数量很多，那么该特征网格就是不可靠的，是动态的，相应的要设置KF中的mask，然后做洪水填充算法，下次可以在mask之外的区域采样光线。\n\n---\n\n**INPUT**: Volume embeding $e=\\{e_i | v_i \\in V\\}$ , Decode network $F_{\\theta}$, KeyFrame Quene Pose $T=\\{T_i\\}$, rgb assets $I=\\{I_i\\}$, depth assets $D=\\{D_i\\}$ \n\n**OUTPUT**: 优化后的 $e$, $F_{\\theta}$, $T$\n\n从KF中的图片中采样光线，得到采样光线集合 $R$. <br>\n记录每一个特征网格被采样光线 $r_i \\in R$ 穿过,记为 $vr=\\{r_i\\}$ <br>\nfor $vr \\in V$ <br>\n&emsp; for $r \\in vr$ <br>\n&emsp; &emsp; 计算是否满足 $|d_s+sdf_{pred}*trunc-d_{gt}|<k*trunc$ <br>\n&emsp; 得到不一致光线的数量 $n$ <br>\n&emsp; if n>N <br>\n&emsp; &emsp; 修改八叉树,标记该网格为动态网格,Vt->Vf, Vs->Vf.强制将Vf中网格的特征向量解码出来的sdf变为trunc,或者将网格设置为透明,不参与光线的求交,自然也不会参与采样等后续计算. <br>\n&emsp; &emsp; 修改关键帧,使用填充算法,去掉关键帧中的动态区域 <br>\n\n---\n\n\n","n":0.091}}},{"i":18,"$":{"0":{"v":"(CVPR 2023)RoDynRF","n":0.707},"1":{"v":"\nRoDynRF： Robust Dynamic Radiance Fields\n\n![系统架构](assets/doctor/RoDynRF_fig2.png){width=100%}\n\n1. 同时重建静态场景nerf和动态场景nerf\n1. 不要求相机位姿和相机内参\n1. 网络设计和损失设计改进位姿估计和动态nerf重建\n1. 在一些困难数据集上也可以正常估计相机位姿，而传统的sfm会失败.\n\n## Method\n\n### 静态重建和相机位姿估计\n\n![Loss](assets/doctor/RoDynRF_fig3.png){width=100%}\n\n1. 场景划分。分为静态区域和动态区域，使用不同的**显示**神经体素网格表示，$\\mathrm{V}^s$ 与 $\\mathrm{V}^s$ 。\n1. 动态区域分割。利用Mask R-CNN和光流法得到的极线距离阈值约束，得到运动物体的掩码。\n1. coarse-to-fine静态场景重建。同时优化输入帧的相机位姿和共享的焦距，使用coarse-to-fine的策略优化静态场景的表示。\n1. 末期视角方向条件。在color mlp的最后一层才混合视角方向。优化时绕过神经体素网格，直接学习视角方向到输出采样颜色的映射函数。\n1. 损失函数。光度损失\n    1. 重投影损失。像素体渲染得到3d点，投影，RAFT寻找对应关系。\n    1. 视差损失。对应点体渲染得到3d点对，计算z差\n    1. 单目深度损失。MiDaSv2.1估计出未知尺度和平移的深度图，约束渲染的深度。\n    1. 优化方法。模拟退火算法，光流估计和深度图估计可能不准确，只在训练的初期参与指导。\n\n### 动态nerf\n\n1. 光度损失\n1. 场景流损失。为了得到3d点的运动，引入了场景流mlp来补偿3d运动。\n    1. 场景流mlp预测的点和形变mlp预测的点的重投影误差、视差、单目深度损失。\n    1. 正则化损失。\n    1. mask损失。\n1. 线性组合。将静态损失和动态损失线性组合起来。\n","n":0.16}}},{"i":19,"$":{"0":{"v":"Point-SLAM","n":1},"1":{"v":"\nPoint-SLAM: Dense Neural Point Cloud-based SLAM\n\n","n":0.408}}},{"i":20,"$":{"0":{"v":"(CVPR 2022 Oral)Point-NeRF","n":0.577},"1":{"v":"\nPoint-NeRF: Point-based Neural Radiance Fields\n\n- 和基于MLP的原版NeRF方法对比。点云模型易扩展、易编辑、模型容量更大。\n- 和基于体素储存特征向量的混合场景表达方式对比。明确模型几何信息，采样和训练效率更高。\n- 和Splatting方法对比。会更慢，但能携带更多的局部信息，拟合view-dependent的效果更好。\n- 和U-Net方法对比。渲染的瓶颈从渲染网格转移到了神经描述子的表达能力上，上限更高。\n\n","n":0.333}}},{"i":21,"$":{"0":{"v":"(CVPR 2022)Nice-SLAM","n":0.707},"1":{"v":"[[(CVPR 2021)iMAP|pub.doctor.nerf.iMAP]]\n","n":0.707}}},{"i":22,"$":{"0":{"v":"(NeurIPS 2021)Neus","n":0.707},"1":{"v":"\n> Learning neural implicit surfaces by volume rendering for multi-viewreconstruction","n":0.316}}},{"i":23,"$":{"0":{"v":"(CVPR 2022)Neural rgb-d surface reconstruction","n":0.447},"1":{"v":"\n提出和tsdf和nerf的混合场景表达，为重建任务量身定做。\n\n![系统](assets/doctor/Neural_RGB-D_Surface_Reconstruction_fig2.png){width=100%}\n\n### 创新点\n\n1. 基于TSDF的Nerf。原版的nerf是基于体密度的表面表示方式会导致大量的artifacts。\n1. 提出了相机位姿refine技术来改进重建效果，预测一个平面形变场，消除<mark>相机畸变和内参估计不准</mark>的问题\n\n> 并不像原版nerf一样基于基于物理的方法，但是可以更好的预测边界，得到更好的重建结果。原版nerf中实际表面的半透明物质可以表示视角依赖的影响，但是在重建任务中，会导致更多的噪声和artifacts。\n\n![结果](assets/doctor/Neural_RGB-D_Surface_Reconstruction_fig3.png){width=100%}\n\nNeRF with Depth： 通过nerf得到的光线的中止点的深度与输入的深度计算L2 Loss。\n\n### 方法\n\n需要使用BundleFusion得到初始的相机位姿。\n\n**颜色权重**\n\n$$\n\\omega_i=\\sigma(\\frac{D_i}{tr}) \\cdot \\sigma(-\\frac{D_i}{tr})\n$$\n\nbell-shaped函数，在物体表面得到的颜色给最大的权重\n\n$$\n\\mathbf{C}=\\frac{1}{\\sum_{i=0}^{K-1}\\omega_i}\\sum_{i=0}^{K-1}\\omega_i \\cdot \\mathbf{c}_i\n$$\n\n### 损失函数\n\n通过sdf直接构建损失函数\n\n$$\n\\begin{gather*} \n\\mathcal{L}_{rgb}^b(\\mathcal{P})=\\frac{1}{|P_b|}\\sum_{p \\in P_b}(C_p-\\hat{C}_p)^2 \\\\\n\\mathcal{L}_{fs}^b(\\mathcal{P})=\\frac{1}{|P_b|}\\sum_{p \\in P_b}\\frac{1}{|S_p^{fs}|}\\sum_{s \\in S_p^{fs}}(D_s-tr)^2 \\\\\n\\mathcal{L}_{tr}^b(\\mathcal{P})=\\frac{1}{P_b}\\sum_{p \\in P_b} \\frac{1}{|S_p^{tr}|} \\sum_{s \\in S_p^{tr}}(D_s-\\hat{D_s})^2\n\\end{gather*}\n$$\n\n预测带符号的距离场\n","n":0.183}}},{"i":24,"$":{"0":{"v":"(CVPR 2021)Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes","n":0.289},"1":{"v":"\n动态前景和背景检测\n","n":1}}},{"i":25,"$":{"0":{"v":"(TOG 2019)Neural Importance Sampling","n":0.5},"1":{"v":"\ninvertible compound function\n\n![coupling layer](assets/doctor/NIS_fig1.png){width=100%}\n\nNIS Pipeline\n\n![Pipline](assets/doctor/NIS_fig3.png){width=100%}\n\n**One-blob Encoding**\n\nkernel函数激活多个相邻的值，而不是单个值（one-hot）\n\n假设一个标量值 $s \\in [0,1]$ ,区间被划分为k个bin($k = 32$)。将核函数（Gaussian kernel with $\\sigma = 1/k,\\mu = s$)离散化到bin中，得到s的one-blob encoding。\n\n","n":0.224}}},{"i":26,"$":{"0":{"v":"(NeurIPS 2021)NSVF","n":0.707},"1":{"v":"\n提出了*Neural Sparse-Voxel Fields(NSVF)*,同时结合了神经隐式场和显示稀疏体素结构的混合场景表达.\n\n1. 使用拒绝采样在光线上采样。\n1. 同时结合了神经隐式场和显示稀疏体素结构的混合场景表达。\n\n![采样方式](assets/doctor/NSVF_fig1.png){width=100%}\n\n","n":0.447}}},{"i":27,"$":{"0":{"v":"(ICCV 2021)Mip-NeRF","n":0.707},"1":{"v":"\nMip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields\n\n","n":0.333}}},{"i":28,"$":{"0":{"v":"(arXiv 2023)MIPS-Fusion","n":0.707}}},{"i":29,"$":{"0":{"v":"(TOG 2022)LaplacianFusion","n":0.707}}},{"i":30,"$":{"0":{"v":"(TOG 2022)Instrant-NGP","n":0.707},"1":{"v":"\nInstant neural graphics primitives with a multiresolution hash encoding\n\n![overview](assets/doctor/iNGP_fig3.png){width=100%}\n\n> Pareto optimum: \n\n## Advantages\n1. \n\n## Background\n\n**高维空间编码** \n- 将输入编码到高维空间，可以使得数据线性可分。可以使用one-hot和kernel trick\n- 输入编码对循环神经网络中的注意力机制也有用\n\n**输入编码方法**\n1. frequency encodings. \n1. one-blob encoding. \n1. parametric encodings. 将可训练的参数放在辅助的数据结构中，空间换时间。\n    - tree。 \n        - 自适应性。\n        - 引入了额外的计算成本。\n    - dense grid。\n        - 浪费空间。\n        - coarse-to-fine。由于自然场景的平滑性，不同的情况使用不同的level.\n1. sparse parametric encodings.\n    - hash grid\n\n## Method\n1. 哈希函数。空间哈希函数\n1. Implicit hash collision resolution. \n    - 不同level同时发生碰撞的概率很低\n    - 不同点的对梯度贡献的权值也不一样\n\n##  Accelerated NERF Ray Marching\n\n1. 步长指数增长。对于大的场景，光线上采样点的数量是对数增长，基本不会造成计算量\n1. 占据网格。使用Occupancy Grids跳过空格子。\n    - 使用。有x和步长delta确定查询第几个网格（网格边长大于x的最小网格）\n    - 更新。训练过程中更新占据网格，每16次训练更新一次。额外使用相同结构的网格记录密度值\n        - 网格单元中密度值以0.95衰减\n        - 随机采样M个候选单元，密度值设为当前值和nerf密度值的最大值\n        - 通过阈值t=0.01*1024*sqrt(3)更新占据网格。\n1. 数据压缩。将采样数据压缩到GPU缓存中，高效的执行。\n","n":0.114}}},{"i":31,"$":{"0":{"v":"(arXiv 2023)H2-Mapping","n":0.707},"1":{"v":"\nH2-Mapping: Real-time Dense Mapping Using Hierarchical Hybrid\nRepresentation\n\n可以在**边缘计算**的设备上实时构建高质量的地图。\n\n- 层次混合表示。利用隐式多分辨率hash编码，描述不同层次的细节。使用了八叉树sdf先验来帮助构建隐式地图，可以加速地图初始化和学习。\n- 最大覆盖关键帧选择策略。解决遗忘问题和提高地图质量。\n- 边缘设备运行。\n\n![Overview](assets/doctor/H2-Mapping_fig2.png)\n\n八叉树节点初始化\n\n**Expanded Voxels Allocation** \n\n当体素顶点sdf接近0时，优化时可能会使sdf符号相反，导致表面不存在。如果多分配一个voxel，优化时总可以保证表面存在。\n\n![Expanded Voxels Allocation](assets/doctor/H2-Mapping_fig4.png)\n\n**Multiresolution Hash Encoding**\n\n学习sdf的残差，而不是完整场景的sdf。\n\n**Coverage-maximizing Keyframe Selection**\n\n$\\frac{N_o}{N_c+N_l}$，$N_c$是当前帧观测到的voxel数量，$N_l$是上一帧的voxel数量，$N_o$是共同的voxel数量。\n\n将voxel标记为unobserverd，从keyframe set优先选择覆盖率最高的K keyframes，将其标记为observed，使用同样的策略应用到unobserverd voxel中，选择K keyframes。如果都observed，则重置为unobserved。\n\n\n","n":0.209}}},{"i":32,"$":{"0":{"v":"(arXiv 2023)Factored NeRF","n":0.577},"1":{"v":"\nFactored Neural Representation for Scene Understanding\n\n## Overview\n\n![pipline](assets/doctor/FactoredNeRF_fig3.png)\n\n## Contributions\n\n## Method \n\n![Network](assets/doctor/FactoredNeRF_fig5.png)\n\n### Initializations\n\n- [1] Fast Online Object Tracking and Segmentation: A Unifying Approach\n- [2] Raft: Recurrent all-pairs field transforms for\noptical flow\n\n使用现有的视觉追踪算法[1]做关键帧的语义标注（实例分割和AABB），然后使用光流算法[2]做关键帧之间的匹配，得到关键帧之间的相对位姿，然后使用BA算法优化关键帧的位姿和语义标注。\n\n### Joint optimization\n\n\n#### Free Space Loss\n\n- [1]Space-time neural irradiance fields for free-viewpoint video\n\n$$\n\\mathcal{L}_{free}(\\mathcal{F})=\\sum_{p \\in P_{free}} |T_p \\alpha_p| / |P_{free}| \\\\\nwhere \\quad P_{free} = \\{p(s,r)|s < D_t(r)\\}\n$$\n\n#### Non-rigid deformation\n\n- [1] Neural surface reconstruction of dynamic scenes with monocular rgb-d camera\n\n","n":0.122}}},{"i":33,"$":{"0":{"v":"(arXiv 2023)ESLAM","n":0.707},"1":{"v":"\n\n在多尺度（coarse | fine）上使用特征平面，通过把隐向量投影到三个坐标平面，可以把空间复杂度从边长的立方降低到平方。\n\n![系统架构](assets/doctor/ESLAM_fig2.png){width=100%}\n\n### 采样\n\n从当前的相机位姿 $T_i=\\{R_i|t_i\\}$ 发出光线，在光线上采样，首先通过分层采样得到 $N_{start}$ 个点，在表面附近通过重要性采样得到 $N_{imp}$ 个点，最终得到采样点集合 ${\\\\{p_n\\\\}}_{n=1}^{N=N_s+N_i}$。\n\n ${\\\\{p_n\\\\}}_{n=1}^{N=N_s+N_i}$\n\n### 体渲染\n\n使用StyleSDF的方法可以将sdf值转换为体密度。\n\n$$\n\\sigma(p_n)=\\frac{1}{\\beta} \\cdot Sigmoid (\\frac{-\\phi_g(p_n)}{\\beta})\n$$\n\n$\\beta$是可学习参数，用来控制表面边界的锐度。\n\n$$\n\\begin{gather*}\n\\omega_n=exp(-\\sum_{k=1}^{n-1}\\mathbf{\\sigma}(p_k))(1-exp(-\\pmb{\\sigma}(p_n))) \\\\\\\\\n\\boldsymbol{\\hat{c}}=\\sum_{n=1}^{N}\\omega_n \\boldsymbol{\\phi_a}(p_n) \\\\\\\\\n\\boldsymbol{\\hat{d}}=\\sum_{n=1}^{N}\\omega_n z_n\n\\end{gather*}\n$$\n\n### 损失函数\n\n","n":0.218}}},{"i":34,"$":{"0":{"v":"(CVPR 2022)DVGO","n":0.707},"1":{"v":"\nDirect Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction\n\nCode: <https://github.com/sunset1995/DirectVoxGO>\n\n**任务** 新视角渲染，提升了收敛速度，并且可以得到高质量的输出。\n\n提出使用稠密体素网格表示几何，使用特征体素网格表示复杂的视角依赖的外观。使用了3种策略来加速收敛\n1. post-activation interpolation。\n1. imposing several priors。\n\n## Method\n\n","n":0.236}}},{"i":35,"$":{"0":{"v":"(CVPR 2023)Co-SLAM","n":0.707},"1":{"v":"\n[[(CVPR 2021)iMAP|pub.doctor.nerf.iMAP]]\n\n[[(CVPR 2022)Nice-SLAM|pub.doctor.nerf.Nice-SLAM]]\n\nCo-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM\n\n![系统架构](assets/doctor/Co-SLAM_fig3.png){width=100%}\n\n1. 模型表示。联合坐标编码和多分辨率稀疏参数编码，用于实时SLAM。\n1. 相机追踪。使用常量运动模型得到当前帧相机位姿的初始位姿。\n1. BA。之前的nerf-based slam存储关键帧，然后在所有关键帧中采样，得到光线，然后计算每一条光线上的损失。而co-slam方法存储的是关键帧中的一部分像素（5%），所以它可以更加频繁的添加关键帧，消融实验说明可以提升精度。\n\n## Method\n\n**坐标编码**\n\n1. 输入编码。One-blob编码方式。$\\gamma(\\mathrm{x})$\n\n**参数化编码**\n\n1. 多分辨率特征网格。$\\mathcal{V}_{\\alpha}=\\{\\mathcal{V_{\\alpha}^l}\\}_{l=1}^{L}$\n1. sdf decoder. $f_{\\tau}(\\gamma(\\mathrm{x}),\\mathcal{V}_{\\alpha}\\mathrm(x)) \\mapsto (\\mathrm{h},s)$\n1. color decoder. $f_{\\phi}(\\gamma(\\mathrm{x}),\\mathrm{h}) \\mapsto \\mathrm{c}$\n\n$\\theta=\\{\\alpha,\\phi,\\tau\\}$是可学习参数。\n\n**采样**\n\n1. 均匀采样。在光线上均匀采样$M$个点，计算color和sdf。\n1. 深度指导采样。重要性采样对于结果并没有重大的提升，反而会减慢追踪和建图的速度。因此，在near和far之间均匀采样 $M_c$ 个点，在表面附近 $[d-d_s,d+d_s]$ 均匀采样 $M_f$ 个点，$d_s$是小的偏移量。\n\n**损失函数**\n\n常规的损失\n$$\n\\mathcal{L}_{rgb}, \\mathcal{L}_{depth}, \\mathcal{L}_{sdf}, \\mathcal{L}_{fs}\n$$\n\n在未被观测的free space空间内，为了防止hash冲突产生噪声，提出了平滑性损失，做为正则化项。\n\n$$\n\\mathcal{L}_{smooth}=\\frac{1}{|\\mathcal{G}|}\\sum_{x \\in \\mathcal{G}} \\Delta_{x}^{2}+\\Delta_{y}^{2}+\\Delta_{z}^{2} \\\\\nwhere \\ \\Delta_{x,y,z}=\\left\\|\\mathcal{V}_{\\alpha}(\\mathcal{x}+\\epsilon_{x,y,z})-\\mathcal{V}_{\\alpha}({\\mathrm{x})}\\right\\|_{2}\n$$\n\n为了保证实时性，在每一次迭代时随即选择一个区域做正则化。\n\n**相机追踪**\n\n恒速假设。camera-to-world变换 $\\mathbf{T}_{wc}=\\mathrm{exp}(\\xi_t^{\\wedge}) \\in \\mathbb{SE}(3)$\n\n$$\n\\mathbf{T}_{t}=\\mathbf{T}_{t-1} \\mathbf{T}_{t-2}^{-1} \\mathbf{T}_{t-1}\n$$\n\n在当前帧选择$N_t$个像素，通过最小化目标函数来优化相机参数\n\n**BA**\n\nBA特点。经典的稠密slam方法做BA需要使用关键帧的所有像素，而神经slam只需要**稀疏的采样**就可以做BA。然而iMap和NICE-SLAM依旧使用原始的关键帧选择策略（基于信息增益、视觉覆盖），关键帧中只有少数的像素（<10%)参与了后续的联合优化。\n\n关键帧存储策略。Co-SLAM中只存储关键帧中的一部分像素（5%），这使得我们可以**频繁的添加**关键帧，维护一个更大的关键帧数据库。执行global BA时，堆积从关键帧数据库选择$N_g$光线。\n\n优化采用**交替**的策略。对场景参数 $\\theta$ 优化 $k_m$次，之后在更新相机位姿参数的累积梯度。由于相机参数只有6个参数，这可以提高相机位姿优化的鲁棒性。\n\n","n":0.132}}},{"i":36,"$":{"0":{"v":"(IROS 2021)BundleTrack","n":0.707},"1":{"v":"\n\n使用GPU实现了实时(10HZ)的的6DoF位姿估计.\n\n## 方法\n\n\n### Mask分割\n\n使用transductive-VOS得到mask掩码\n\n### 关键点检测/匹配/配准\n\n","n":0.5}}},{"i":37,"$":{"0":{"v":"(CVPR 2023)BundleSDF","n":0.707},"1":{"v":"\n相机固定,对刚性物体做追踪重建.也可以用在静态场景重建的SLAM中.\n\n![系统架构](assets/doctor/BundleSDF_fig2.png){width=100%}\n\n## 方法\n\n###  Online Pose Graph Optimization\n\n从内存池(SLAM中叫关键帧队列)取 $|\\mathcal{P}_{pg}|=K$ 帧,与当前帧 $\\mathcal{F}_t$ 构成位姿图 $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$,即 $\\mathcal{V}=\\mathcal{F}_t \\cup \\mathcal{P}_{pg}$. \n\n位姿图损失函数\n\n$$\n\\mathcal{L}_{pg}=\\omega_s \\mathcal{L}_{s}(t)+\\sum_{i \\in \\mathcal{V},j \\in \\mathcal{V},i \\neq j}[\\omega_f \\mathcal{L}_{f}(i,j)+\\omega_p \\mathcal{L}_{p}(i,j)] \n$$\n\n$\\omega_s,\\omega_f, \\omega_p$ 经验性的设置为1.\n\n$$\n\\mathcal{L}_{f}(i,j)=\\sum_{(p_m,p_n) \\in C_{i,j}} \\rho (||\\xi_{i}^{-1}p_m-\\xi_{j}^{-1}p_n||_2)\n$$\n\n$C_{i,j}$是通过网络检测出来的对应关系.\n\n$$\n\\mathcal{L}_{p}(i,j)=\\sum_{p \\in I_i} \\rho (|n_i(p) \\cdot (T_{ij}^{-1} \\pi_{D_j}^{-1}(\\pi_j(T_{ij}p))-p)|)\n$$\n\n$\\mathcal{F}^{(i)}$ 中的点在 $\\mathcal{F}^{(j)}$ 中的重投影误差.\n\n$$\n\\mathcal{L}_{s}(t)=\\sum_{p \\in I_t} \\rho (|\\Omega (\\xi_t^{-1}(\\pi_D^{-1}(p)))|)\n$$\n\n深度图中点的在网络 $\\Omega$ 中查询出来的sdf值为0.\n\n### Neural Object Field\n\n![神经物体场](assets/doctor/BundleSDF_fig3.png){width=100%}\n\n几何函数 $\\Omega:x \\mapsto s,x \\in \\mathbb{R}^3,s \\in \\mathbb{R}$\n\n外观函数 $\\Phi:(f_{\\Omega(x)},n,d) \\mapsto c, f_{\\Omega(x)} \\in \\mathbb{R}^3,n \\in \\mathbb{R}^3,d \\in \\mathbb{R}^3,c \\in \\mathbb{R}_{+}^3$\n\n法线 $n(x)=\\frac{\\partial \\Omega(x)}{\\partial x}$\n\n光线ray $x_i(r)=o(r)+t_i d(r)$ $o(r),d(r)$是光线的原点和方向\n\n外观渲染方程\n\n$$\\begin{gather*}\nc(r)=\\int_{z(r)-\\lambda}^{z(r)+0.5 \\lambda} \\omega(x_i) \\Phi(f_\\Omega(x_i),n(x_i),d(x_i)) \\mathrm{d}t \\\\\n\\omega(x_i)=\\frac{1}{1+e^{-\\alpha \\Omega(x_i)}}\\frac{1}{1+e^{\\alpha \\Omega(x_i)}}\n\\end{gather*}$$\n\n$\\omega(x_i)$ 是钟形概率密度函数. $\\alpha$ 常数, 用来控制概率分布.\n\n采样策略 在**空间占据网格开始边界**到 $z(r)+0.5 \\lambda$ 之间采样 $N$ 个点, 再以概率 $\\mathcal{N}(z(r),\\lambda^2)$ 采样 $N'$ 个点, 最终采样 $N+N'$ 个点.\n\n*Uncertain free space*不确定性空间损失,对于边界处的采样光线,给一个小值.\n\n$$\n\\mathcal{L}_u=\\frac{1}{|\\mathcal{X}_u|}\\sum_{x \\in \\mathcal{X}_u}(\\Omega(x)-\\epsilon)^2\n$$\n\n*Empty(free) space* 自由空间损失\n\n$$\n\\mathcal{L}_e=\\frac{1}{|\\mathcal{X}_e|}\\sum_{x \\in \\mathcal{X}_e}|\\Omega(x)-\\lambda|\n$$\n\n*Near-surface space* 近表面空间损失\n\n$$\\begin{gather*}\n\\mathcal{L}_{surf}=\\frac{1}{|\\mathcal{X}_{surf}|}\\sum_{x \\in \\mathcal{X}_{surf}}(\\Omega(x)+d_x-d_D)^2 \\\\\nd_x=||x-o(r)||_2 \\\\\nd_D=||\\pi^{-1}(z(r))||_2\n\\end{gather*}$$\n\n*appearance network supervision* 颜色损失\n\n$$\n\\mathcal{L}_c=\\frac{1}{|\\mathcal{R}|} \\sum_{r \\in \\mathcal{R}}||\\Phi(f_\\Omega(x),n(x),d(x))-\\bar{c}(r)||_2\n$$\n\n*Eikonal regularization* Eikonal 正则化,用来保证网络预测的值为sdf\n\n$$\n\\mathcal{L}_{eik}=\\frac{1}{|\\mathcal{X}_{surface}|} \\sum_{x \\in \\mathcal{X}_{surface}}(||\\nabla \\Omega(x)||_2-1)^2\n$$\n\n训练损失函数\n\n$$\n\\mathcal{L}=\\mathcal{L}_{pg}+\\lambda_u \\mathcal{L}_u+\\lambda_e \\mathcal{L}_e+\\lambda_{surf} \\mathcal{L}_{surf}+\\lambda_c \\mathcal{L}_c+\\lambda_{eik} \\mathcal{L}_{eik}\n$$\n\n可训练参数为resolution hash encoder, $\\Omega$ , $\\Phi$ , and the object **pose updates** in the tangent space parametrized in Lie Algebra $\\Delta\\bar{\\xi} \\in \\mathbb{R}^{(|\\mathcal{P}-1|\\times 6)}$, 第一帧的位姿固定.\n","n":0.08}}},{"i":38,"$":{"0":{"v":"Math","n":1}}},{"i":39,"$":{"0":{"v":"Tensor","n":1},"1":{"v":"\n- [知乎：彻底理解张量(上)](https://zhuanlan.zhihu.com/p/508715535)\n- [知乎：彻底理解张量(下)](https://zhuanlan.zhihu.com/p/508715717)\n- [知乎：彻底理解张量拓展](https://zhuanlan.zhihu.com/p/565588180)\n\n## 线性空间\n\n## 对偶空间\n\n## 张量\n\n##\n\n","n":0.378}}},{"i":40,"$":{"0":{"v":"Sampling","n":1},"1":{"v":"\n## inverse CDF sampling\n\n> 使用累积分布函数做变换得到的随即变量服从(0,1)上的均匀分布。反之，如果先从（0,1）上的均匀分布采样y,在做某个累积分布的逆变换，得到的累积分布函数正好是 $F_X(x)$ \n\ncdf的逆函数: $x=F_X^{-1}(u)=\\inf_x\\{x:F_X(x)>u\\}$\n\n生成cdf为 $F_X(x)$ 样本的过程：\n\n1. 生成 $U \\sim U(0,1)$ 的随即变量 $U$。\n1. 计算 $X=F_X^{-1}(U)$\n\n即证明 $P(X \\leq x)=F_X(x)$ .\n\n$P(X \\leq x)=P(F_X^{-1}(U) \\leq x)=P(U \\leq F_X(x))=F_U(F_X(x))=F_X(x)$\n\n## Rejection sampling\n\n\n\n## Importance sampling\n\n## Metropolis-Hastings sampling\n\n## Gibbs sampling\n\n\n## MCMC sampling\n","n":0.162}}},{"i":41,"$":{"0":{"v":"Reparameterization","n":1},"1":{"v":"\n\n- [参考链接1](https://zhuanlan.zhihu.com/p/344938643)\n- [参考链接2](https://zhuanlan.zhihu.com/p/364178598)\n\n**重参数化技巧** 分离随机变量的不确定性，使得原先无法求导/梯度传播的中间节点可以求导。\n\n## Contiguous latent variable in VAE\n\n直接建模两个Contiguous variable之间的转换函数 $g$ 是很复杂的一件事，但两个协方差矩阵为对角矩阵的多维高斯分布之间的转换却是很简单的，这也是为什么VAE中采用高斯分布对Latent variable建模。\n\n$$\nz=g_{\\theta}(\\epsilon)=\\mu_{\\theta}+\\sigma_{\\theta}\\epsilon\n$$\n\n## Gumbel softmax trick\n\n- [参考链接](https://zhuanlan.zhihu.com/p/455084077)","n":0.243}}},{"i":42,"$":{"0":{"v":"Probability","n":1},"1":{"v":"\n## 动态贝叶斯网络\n\n\n## 因子图\n\n\n## 贝叶斯\n\n## 移动概率\n\n","n":0.447}}},{"i":43,"$":{"0":{"v":"Optimation","n":1},"1":{"v":"\n## 高斯牛顿优化","n":0.707}}},{"i":44,"$":{"0":{"v":"Neural","n":1},"1":{"v":"\n## sigmoid \n\n>sigmod的函数是一个在生物学中常见的S型函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，常被用作神经网络的激活函数，将变量映射到0,1之间。-------------摘自《百度百科》\n\n优点：平滑，易于求导\n\n缺点：计算量大，梯度消失导致深层网络无法训练\n\n计算公式 $f(x)=\\frac{1}{1+e^{-x}}$,定义域为R,值域为$(0,1)$,单增。导数为$f'(x)=\\frac{e^{-x}}{(1+e^{-x})^2}=f(x)(1-f(x))$,定义域为R,值域为$(0,0.25)$，x=0轴对称，bell-shaped函数\n\n## PyTorch\n\n![pytorch grad](assets/science/pytorch_grad.png)\n\n","n":0.408}}},{"i":45,"$":{"0":{"v":"Intersection","n":1},"1":{"v":"\n## Ray and AABB\n\n\n## Ray and Triangle\n\n\n## Ray and Sphere","n":0.316}}},{"i":46,"$":{"0":{"v":"Estimation","n":1},"1":{"v":"\n## Image\n\n图像相似度 \n> Peak Signal-to-Noise Ratio(PSNR): PSNR作为一种图像质量评估指标，对于人眼感知的图像质量并不总是准确，因为它仅考虑了图像的均方误差，而忽略了人眼对不同频率成分的敏感度差异。因此，在实际应用中，还可以结合其他指标或进行主观评估来综合评估图像质量。\n\n计算公式 $PSNR = 10 * log10((MAX^2) / MSE)$, 其中$MSE = (1 / (M * N)) * \\sum_i [\\sum_j ((I(i,j) - K(i,j))^2)]$, M和N分别表示图像的宽度和高度，I(i,j)表示原始图像的像素值，K(i,j)表示经过处理后的图像的像素值。\n","n":0.189}}},{"i":47,"$":{"0":{"v":"English","n":1},"1":{"v":"\n# 听力\n\n1. 选择生活化的美剧，熟悉之后，可以选择专业内容方面的公开课，有声书等等。\n\n1. 可理解性的听力输入，先理解一段话的意思，然后再听懂每一句话。\n\n1. 不要复习，看着画面或者字幕理解意思以后，关闭字幕再听一遍。不要再在短时间听重复的内容，会大量浪费时间。\n","n":0.447}}},{"i":48,"$":{"0":{"v":"Tools","n":1}}},{"i":49,"$":{"0":{"v":"Openai","n":1},"1":{"v":"\n# Chatgpt Prompt\n\n## English\n\n### Translation\n\n**CN to EN**\n\n> Please translate following sentence to English:XXX\n\n**Academic Translation**\n\n> I want you to act as a scientific English-Chinese translator, I will provide you with some paragraphs in one language and your task is to accurately and academically translate the paragraphs only into the other language.Do not repeat the original provided paragraphs after translation.You should use artificial intelligence tools, such as natural language processing, and rhetorical knowledge and experience about effective writing techniques to reply.I'll give you my paragraphs as follows, tell me what language it is written in, and then translate:XXX\n\n\n### Improve\n\n**Sentence**\n\n> Below is a sentence.  Help me improve the spelling, grammar, clarity, concision and overall readability. When necessary, rewrite the whole sentence\n> \n>\n\n**Academic**\n\n> Below is a paragraph from an academic paper.Polish the writing to meet the academic style,improve the spelling, grammar, clarity, concision and overall readability.When necessary, rewrite the whole sentence.Furthermore, list all modification and explain the reasons to do so in markdown table.Paragraph ：XXX\n\n**Grammer**\n\n> Can you help me ensure that the grammar and the spelling is correct?Do not try to polish the text, if no mistake is found, tell me that this paragraph is good.If you find grammar or spelling mistakes, please list mistakes you find in a two-column markdown table, put the original text the first column, put the corrected text in the second column and highlight the key words you fixed.Example: Paragraph: How is you?Do you knows what is it?|Original sentence |Corrected sentence ||:--- |:--- ||How is you?|How are you?||Do you knows what is it?|Do you know what it is ?|Below is a paragraph from an academic paper.You need to report all grammar and spelling mistakes as the example before.Paragraph: XXX\n\n\n## Utils\n\n### Act as a Prompt Generator\n\n> I want you to act as a prompt generator. Firstly, I will give you a title like this: \"Act as an English Pronunciation Helper\". Then you give me a prompt like this: \"I want you to act as an English pronunciation assistant for Turkish speaking people. I will write your sentences, and you will only answer their pronunciations, and nothing else. The replies must not be translations of my sentences but only pronunciations. Pronunciations should use Turkish Latin letters for phonetics. Do not write explanations on replies. My first sentence is \"how the weather is in Istanbul?\".\" (You should adapt the sample prompt according to the title I gave. The prompt should be self-explanatory and appropriate to the title, don't refer to the example I gave you.). My first title is \"Act as a Code Review Helper\" (Give me prompt only)\n\n","n":0.048}}},{"i":50,"$":{"0":{"v":"Arch","n":1},"1":{"v":"\n# Install\n\n\n\n\n# SoftWare\n\n## pacman \n\n**[清华源](https://mirrors.tuna.tsinghua.edu.cn/help/archlinuxcn/)**\n\n在/etc/pacman.conf文件末尾添加以下两行：\n```\n[archlinuxcn]\nServer = https://mirrors.tuna.tsinghua.edu.cn/archlinuxcn/$arch\n```\n之后安装 archlinuxcn-keyring 包导入 GPG key。\n\n修改/etc/pacman.conf，增加32位软件源\n```\n[multilib]\nInclude = /etc/pacman.d/mirrorlist\n```\n执行 ```sudo pacman -Syy ```\n\n软件管理\n```bash\n# 更新系统\nsudo pacman -Syy && sudo pacman -Su\n# 安装软件\nsudo pacman -S <pkg name>\n# 卸载软件\nsudo pacman -R <pkg-name>\n# 查询文件属于哪个包管理器\nsudo pacman -Qo /path/to/file_name\n# \n```\n\n阻止软件升级\n```\n# /etc/pacman.conf,修改以下行\n[options]\nIgnorePkg = <pkg name>\n```\n\n## yay \n\n\n\n## kde\n\n[Arch kde install](https://wiki.archlinux.org/title/KDE)\n\nkde-plasma,plasma-desktop\n\n系统监视器 system info\n\n截图 spectacle\n\n文件夹 Dolphin\n\n网络管理工具 plasma-nm\n\n## fcitx5\n\n安装[fcitx5](https://wiki.archlinux.org/title/Fcitx5)\n```bash\nsudo pacman -S fcitx5 fcitx5-im fcitx5-qt fcitx5-gtk fcitx5-chinese-addons\n```\n\n编辑环境变量/etc/environment\n```\nINPUT_METHOD=fcitx5\nGTK_IM_MOUDLE=fcitx5\nQT_IM_MODULE=fcitx5\nXMODIFIERS=@im=fcitx5\n```\n\n## ssh\n\nssh连接3090失败，No route to host\n\n1. 重启NetworkManager\n```bash\n# sudo systemctl restart NetworkManager\n\nping -I <ip or interface name> <dest>\n# 查看路由表,添加路由，编辑/etc/profile\nroute -n\nsudo route add -net 10.184.0.0 netmask 255.255.0.0 gw 10.181.0.1\n``` \n\n## nvidia\n\n[cudatookits](https://developer.nvidia.com/cuda-downloads)\n\n## python\n### anaconda\n\n安装地址 [Aur](https://aur.archlinux.org/packages/anaconda)\n\n\n#### conda换[清华源](https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/)\n\nTUNA 还提供了 Anaconda 仓库与第三方源（conda-forge、msys2、pytorch等，查看完整列表，更多第三方源可以前往校园网联合镜像站查看）的镜像，各系统都可以通过修改用户目录下的 .condarc 文件来使用 TUNA 镜像源。Windows 用户无法直接创建名为 .condarc 的文件，可先执行 conda config --set show_channel_urls yes 生成该文件之后再修改。\n\n注：由于更新过快难以同步，我们不同步pytorch-nightly, pytorch-nightly-cpu, ignite-nightly这三个包。\n\n```yaml\nchannels:\n  - defaults\nshow_channel_urls: true\ndefault_channels:\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2\ncustom_channels:\n  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n```\n运行 conda clean -i 清除索引缓存，保证用的是镜像站提供的索引。\n\n运行 conda create -n myenv numpy 测试一下吧。\n\n#### pip换[清华源](https://mirrors.tuna.tsinghua.edu.cn/help/pypi/)\n\n临时使用\n```bash\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package\n```\n\n设为默认\n```bash\npython -m pip install --upgrade pip\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n# 或者如果您到 pip 默认源的网络连接较差，临时使用本镜像站来升级 pip：\npython -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip\n```\n配置多个镜像源。如果您想配置多个镜像源平衡负载，可在已经替换 index-url 的情况下通过以下方式继续增加源站：\n```bash\n# 请自行替换引号内的内容，源地址之间需要有空格\npip config set global.extra-index-url \"<url1> <url2>...\"\n```\n可用的 pypi 源列表（校园网联合镜像站): (https://mirrors.cernet.edu.cn/list/pypi)\n\n## cpp\n### clang \n\n### cmake \n\n### g++\n\n### cmake \n\n\n## htop\n\n监视进程命令行工具\n\n## fish\n\nfish, the friendly interactive shell, is a commandline shell intended to be interactive and user-friendly.\n\n[docs](https://fishshell.com/docs/current/index.html)\n\n### 环境变量设置\n第一种：修改~/.config/fish/config.fish\n```bash\nset -x PATH /opt/cuda/bin $PATH\n```\n\n第二种\n```bash \nfish_add_path <path>\n```\n\n第三种：修改~/.config/fish/fish_variables\n\n### 命令提示符添加git支持\n```bash\nfunction fish_prompt --description 'Write out the prompt'\n    set -l last_pipestatus $pipestatus\n    set -g __fish_git_prompt_showupstream verbose\n    set -g __fish_git_prompt_showcolorhints\n    set -g __fish_git_prompt_show_informative_status\n    set -g fish_prompt_hg_show_informative_status\n    set -lx __fish_last_status $status # Export for __fish_print_pipestatus.\n    set -l normal (set_color normal)\n    set -q fish_color_status\n    or set -g fish_color_status --background=red white\n\n    # Color the prompt differently when we're root\n    set -l color_cwd $fish_color_cwd\n    set -l suffix '>'\n    if functions -q fish_is_root_user; and fish_is_root_user\n        if set -q fish_color_cwd_root\n            set color_cwd $fish_color_cwd_root\n        end\n        set suffix '#'\n    end\n\n    # Write pipestatus\n    # If the status was carried over (if no command is issued or if `set` leaves the status untouched), don't bold it.\n    set -l bold_flag --bold\n    set -q __fish_prompt_status_generation; or set -g __fish_prompt_status_generation $status_generation\n    if test $__fish_prompt_status_generation = $status_generation\n        set bold_flag\n    end\n    set __fish_prompt_status_generation $status_generation\n    set -l status_color (set_color $fish_color_status)\n    set -l statusb_color (set_color $bold_flag $fish_color_status)\n    set -l prompt_status (__fish_print_pipestatus \"[\" \"]\" \"|\" \"$status_color\" \"$statusb_color\" $last_pipestatus)\n\n    echo -n -s (prompt_login)' ' (set_color $color_cwd) (prompt_pwd) $normal (fish_vcs_prompt) $normal \" \"$prompt_status $suffix \" \"\nend\n```\n\n## v2raya \n\n版本选择<=4\n\n阻止pacman更新v2ray-core\n\n## Tencent\n\n使用wine安装微信\n\n```bash\n# locale -a should have zh_CN.utf8\nlocale -a\n# 修改/etc/locale.gen，去掉zh_CN.UTF-8 UTF-8前面的注释\nsudo vim /etc/locale.gen\n# 生成locale\nsudo locale-gen\n\n# 安装wine-for-wechat from archlinuxcn\nsudo pacman -S wine-for-wechat\n# 下载wechatsetup.exe\nwine64 WechatSetup.exe\n# 安装winetricks\nsudo pacman -S winetricks\n# 安装微信所需的依赖，处理没有输入框的情况\nwinetricks riched20\n\n# 将msyh字体复制到/usr/share/fonts/msyh文件夹下，修复乱码\nmkfontscale\nmkfontdir\nfc-cache -fv\n\n# 微信窗口关闭在打开就不能输入中文了\n\n```\n\n## Microsoft\n\n[onedriver github](https://github.com/jstaf/onedriver)\n\nopenai block bwg server\n  \n```bash \n# 安装cloudflare warp， https://pkg.cloudflareclient.com/install\ncurl https://pkg.cloudflareclient.com/pubkey.gpg | sudo gpg --yes --dearmor --output /usr/share/keyrings/cloudflare-warp-archive-keyring.gpg\n\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/cloudflare-warp-archive-keyring.gpg] https://pkg.cloudflareclient.com/ $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/cloudflare-client.list\n\nsudo apt-get update && sudo apt-get install cloudflare-warp\n\n# set warp\nsudo warp-cli register\nsudo warp-cli set-mode proxy\nsudo warp-cli connect\ncurl ifconfig.me --proxy socks5://127.0.0.1:40000\ncurl caht.openai.com --proxy socks5://127.0.0.1:40000\n\n```\n\n修改服务器trojan入站出战规则（/etc/v2ray/config.json）\n```json\n \"outbounds\": [{\n    \"protocol\": \"freedom\",\n    \"settings\": {}\n  },{\n    \"protocol\": \"blackhole\",\n    \"settings\": {},\n    \"tag\": \"blocked\"\n  },{\n    \"tag\": \"chatGPT_proxy\",\n    \"protocol\": \"socks\",\n    \"settings\": {\n      \"servers\": [\n        {\n          \"address\": \"127.0.0.1\",\n          \"port\": 40000\n        }\n      ]\n    }\n  }],\n  \"routing\":{\n    \"rules\":[\n      {\n        \"type\": \"field\",\n        \"outboundTag\": \"chatGPT_proxy\",\n        \"domain\": [\n          \"chat.openai.com\",\n          \"ip138.com\",\n          \"domain:openai.com\",\n          \"domain:ai.com\"\n        ]\n      }\n    ]\n  }\n```\n\n## wps\n\n## zotero\n\n**plugin**\n\n- ZotFile <https://github.com/jlegewie/zotfile.git>\n- Zotero Tag <https://github.com/windingwind/zotero-tag.git>\n- Zutilo Utility for Zotero <https://github.com/wshanks/Zutilo.git>\n- Zotero Scholar Citations <https://github.com/nico-zck/zotero-scholar-citations.git>\n- Better Notes for Zotero <https://github.com/windingwind/zotero-better-notes.git>\n- Zotero GPT <https://github.com/MuiseDestiny/zotero-gpt.git>\n- Zotero Style <https://github.com/MuiseDestiny/zotero-style.git>\n- Zotero Refrence <https://github.com/MuiseDestiny/zotero-reference.git>\n\n## meshlab\n\n## zeno\n\n# Code\n\n## word abbreviation\n\nrules \n1. The first few or some letters of a word, such as addr (address) and asm (assembly);\n1. Word syllable initials splicing, such as msg (message);\n1. Multiple word initials splicing, such as NASA (National Aeronautics and Space Administration);\n1. Remove the vowels aeiou and keep the consonants, such ad JPN (Japan), CHN (China) and ft (foot);\n1. A common abbreviation, such as thx(thanks).\n\n","n":0.041}}}]}
