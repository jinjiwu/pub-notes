<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/pub-notes/favicon.ico"/><title>(CVPR 2023)RoDynRF</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal Knowledge Space"/><meta property="og:title" content="(CVPR 2023)RoDynRF"/><meta property="og:description" content="Personal Knowledge Space"/><meta property="og:url" content="https://jinjiwu.github.io/pub-notes/notes/60dx3g7g8bkl9m078xuvg2z/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="6/13/2023"/><meta property="article:modified_time" content="6/13/2023"/><link rel="canonical" href="https://jinjiwu.github.io/pub-notes/notes/60dx3g7g8bkl9m078xuvg2z/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/pub-notes/_next/static/css/b769a59bc6817f50.css" as="style"/><link rel="stylesheet" href="/pub-notes/_next/static/css/b769a59bc6817f50.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/pub-notes/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/pub-notes/_next/static/chunks/webpack-fe3d49582244d28e.js" defer=""></script><script src="/pub-notes/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/pub-notes/_next/static/chunks/main-b76cc11bc9fd1fd4.js" defer=""></script><script src="/pub-notes/_next/static/chunks/pages/_app-8493a92dac9b9de7.js" defer=""></script><script src="/pub-notes/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/pub-notes/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/pub-notes/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/pub-notes/_next/static/DPk29Lz7PV3gbz_lK8oCu/_buildManifest.js" defer=""></script><script src="/pub-notes/_next/static/DPk29Lz7PV3gbz_lK8oCu/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="cvpr-2023rodynrf">(CVPR 2023)RoDynRF<a aria-hidden="true" class="anchor-heading icon-link" href="#cvpr-2023rodynrf"></a></h1>
<p>RoDynRF： Robust Dynamic Radiance Fields</p>
<p><img src="/pub-notes/assets/doctor/RoDynRF_fig2.png" alt="系统架构" style=""></p>
<ol>
<li>同时重建静态场景nerf和动态场景nerf</li>
<li>不要求相机位姿和相机内参</li>
<li>网络设计和损失设计改进位姿估计和动态nerf重建</li>
<li>在一些困难数据集上也可以正常估计相机位姿，而传统的sfm会失败.</li>
</ol>
<h2 id="method">Method<a aria-hidden="true" class="anchor-heading icon-link" href="#method"></a></h2>
<h3 id="静态重建和相机位姿估计">静态重建和相机位姿估计<a aria-hidden="true" class="anchor-heading icon-link" href="#静态重建和相机位姿估计"></a></h3>
<p><img src="/pub-notes/assets/doctor/RoDynRF_fig3.png" alt="Loss" style=""></p>
<ol>
<li>场景划分。分为静态区域和动态区域，使用不同的<strong>显示</strong>神经体素网格表示，<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="normal">V</mi><mi>s</mi></msup></mrow><annotation encoding="application/x-tex">\mathrm{V}^s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span></span></span></span></span></span></span></span></span> 与 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="normal">V</mi><mi>s</mi></msup></mrow><annotation encoding="application/x-tex">\mathrm{V}^s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span></span></span></span></span></span></span></span></span> 。</li>
<li>动态区域分割。利用Mask R-CNN和光流法得到的极线距离阈值约束，得到运动物体的掩码。</li>
<li>coarse-to-fine静态场景重建。同时优化输入帧的相机位姿和共享的焦距，使用coarse-to-fine的策略优化静态场景的表示。</li>
<li>末期视角方向条件。在color mlp的最后一层才混合视角方向。优化时绕过神经体素网格，直接学习视角方向到输出采样颜色的映射函数。</li>
<li>损失函数。光度损失
<ol>
<li>重投影损失。像素体渲染得到3d点，投影，RAFT寻找对应关系。</li>
<li>视差损失。对应点体渲染得到3d点对，计算z差</li>
<li>单目深度损失。MiDaSv2.1估计出未知尺度和平移的深度图，约束渲染的深度。</li>
<li>优化方法。模拟退火算法，光流估计和深度图估计可能不准确，只在训练的初期参与指导。</li>
</ol>
</li>
</ol>
<h3 id="动态nerf">动态nerf<a aria-hidden="true" class="anchor-heading icon-link" href="#动态nerf"></a></h3>
<ol>
<li>光度损失</li>
<li>场景流损失。为了得到3d点的运动，引入了场景流mlp来补偿3d运动。
<ol>
<li>场景流mlp预测的点和形变mlp预测的点的重投影误差、视差、单目深度损失。</li>
<li>正则化损失。</li>
<li>mask损失。</li>
</ol>
</li>
<li>线性组合。将静态损失和动态损失线性组合起来。</li>
</ol></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#method" title="Method">Method</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#静态重建和相机位姿估计" title="静态重建和相机位姿估计">静态重建和相机位姿估计</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#动态nerf" title="动态nerf">动态nerf</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"60dx3g7g8bkl9m078xuvg2z","title":"(CVPR 2023)RoDynRF","desc":"","updated":1686675898739,"created":1686673223650,"custom":{},"fname":"pub.doctor.nerf.RoDynRF","type":"note","vault":{"fsPath":".","selfContained":true,"name":"notes"},"contentHash":"871de0c0fb4b3b679759226092d15ef6","links":[],"anchors":{"method":{"type":"header","text":"Method","value":"method","line":17,"column":0,"depth":2},"静态重建和相机位姿估计":{"type":"header","text":"静态重建和相机位姿估计","value":"静态重建和相机位姿估计","line":19,"column":0,"depth":3},"动态nerf":{"type":"header","text":"动态nerf","value":"动态nerf","line":33,"column":0,"depth":3}},"children":[],"parent":"skbhp29pbdk24509du459z4","data":{}},"body":"\u003ch1 id=\"cvpr-2023rodynrf\"\u003e(CVPR 2023)RoDynRF\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#cvpr-2023rodynrf\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eRoDynRF： Robust Dynamic Radiance Fields\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/pub-notes/assets/doctor/RoDynRF_fig2.png\" alt=\"系统架构\" style=\"\"\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e同时重建静态场景nerf和动态场景nerf\u003c/li\u003e\n\u003cli\u003e不要求相机位姿和相机内参\u003c/li\u003e\n\u003cli\u003e网络设计和损失设计改进位姿估计和动态nerf重建\u003c/li\u003e\n\u003cli\u003e在一些困难数据集上也可以正常估计相机位姿，而传统的sfm会失败.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"method\"\u003eMethod\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#method\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"静态重建和相机位姿估计\"\u003e静态重建和相机位姿估计\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#静态重建和相机位姿估计\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/pub-notes/assets/doctor/RoDynRF_fig3.png\" alt=\"Loss\" style=\"\"\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e场景划分。分为静态区域和动态区域，使用不同的\u003cstrong\u003e显示\u003c/strong\u003e神经体素网格表示，\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi mathvariant=\"normal\"\u003eV\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathrm{V}^s\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathrm\" style=\"margin-right:0.01389em;\"\u003eV\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.6644em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003es\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e 与 \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi mathvariant=\"normal\"\u003eV\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathrm{V}^s\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathrm\" style=\"margin-right:0.01389em;\"\u003eV\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.6644em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003es\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e 。\u003c/li\u003e\n\u003cli\u003e动态区域分割。利用Mask R-CNN和光流法得到的极线距离阈值约束，得到运动物体的掩码。\u003c/li\u003e\n\u003cli\u003ecoarse-to-fine静态场景重建。同时优化输入帧的相机位姿和共享的焦距，使用coarse-to-fine的策略优化静态场景的表示。\u003c/li\u003e\n\u003cli\u003e末期视角方向条件。在color mlp的最后一层才混合视角方向。优化时绕过神经体素网格，直接学习视角方向到输出采样颜色的映射函数。\u003c/li\u003e\n\u003cli\u003e损失函数。光度损失\n\u003col\u003e\n\u003cli\u003e重投影损失。像素体渲染得到3d点，投影，RAFT寻找对应关系。\u003c/li\u003e\n\u003cli\u003e视差损失。对应点体渲染得到3d点对，计算z差\u003c/li\u003e\n\u003cli\u003e单目深度损失。MiDaSv2.1估计出未知尺度和平移的深度图，约束渲染的深度。\u003c/li\u003e\n\u003cli\u003e优化方法。模拟退火算法，光流估计和深度图估计可能不准确，只在训练的初期参与指导。\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"动态nerf\"\u003e动态nerf\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#动态nerf\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e光度损失\u003c/li\u003e\n\u003cli\u003e场景流损失。为了得到3d点的运动，引入了场景流mlp来补偿3d运动。\n\u003col\u003e\n\u003cli\u003e场景流mlp预测的点和形变mlp预测的点的重投影误差、视差、单目深度损失。\u003c/li\u003e\n\u003cli\u003e正则化损失。\u003c/li\u003e\n\u003cli\u003emask损失。\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e线性组合。将静态损失和动态损失线性组合起来。\u003c/li\u003e\n\u003c/ol\u003e","noteIndex":{"fname":"pub","stub":true,"vault":{"fsPath":".","selfContained":true,"name":"notes"},"schemaStub":false,"type":"note","updated":1698572854809,"created":1698572854809,"id":"3w8ufj0ddqin6wp485hfdsg","desc":"","links":[],"anchors":{},"children":["9rgzbgkb65netbhrh58dxnz","hhz8q9pblj7dmva0w1jijb8"],"parent":null,"body":"","data":{},"title":"Pub","custom":{"nav_order":0,"permalink":"/"}},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"notes"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false},"preview":{"theme":"custom","enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["pub"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal Knowledge Space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"master","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"search","theme":"dark","usePrettyRefs":true,"assetsPrefix":"/pub-notes","siteUrl":"https://jinjiwu.github.io","siteFaviconPath":"favicon.ico","siteIndex":"pub"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"60dx3g7g8bkl9m078xuvg2z"},"buildId":"DPk29Lz7PV3gbz_lK8oCu","assetPrefix":"/pub-notes","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>