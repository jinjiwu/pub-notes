<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/pub-notes/favicon.ico"/><title>Review</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal Knowledge Space"/><meta property="og:title" content="Review"/><meta property="og:description" content="Personal Knowledge Space"/><meta property="og:url" content="https://jinjiwu.github.io/pub-notes/notes/xg7qa72kkuq1elnh2ewtsqv/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="3/1/2023"/><meta property="article:modified_time" content="8/24/2023"/><link rel="canonical" href="https://jinjiwu.github.io/pub-notes/notes/xg7qa72kkuq1elnh2ewtsqv/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/pub-notes/_next/static/css/b769a59bc6817f50.css" as="style"/><link rel="stylesheet" href="/pub-notes/_next/static/css/b769a59bc6817f50.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/pub-notes/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/pub-notes/_next/static/chunks/webpack-fe3d49582244d28e.js" defer=""></script><script src="/pub-notes/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/pub-notes/_next/static/chunks/main-b76cc11bc9fd1fd4.js" defer=""></script><script src="/pub-notes/_next/static/chunks/pages/_app-8493a92dac9b9de7.js" defer=""></script><script src="/pub-notes/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/pub-notes/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/pub-notes/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/pub-notes/_next/static/DPk29Lz7PV3gbz_lK8oCu/_buildManifest.js" defer=""></script><script src="/pub-notes/_next/static/DPk29Lz7PV3gbz_lK8oCu/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="review">Review<a aria-hidden="true" class="anchor-heading icon-link" href="#review"></a></h1>
<h1 id="论文整理">论文整理<a aria-hidden="true" class="anchor-heading icon-link" href="#论文整理"></a></h1>
<h2 id="传统方法">传统方法<a aria-hidden="true" class="anchor-heading icon-link" href="#传统方法"></a></h2>
<div class="table-responsive">




















































<table><thead><tr><th>论文</th><th>解决的问题</th><th>采用的方法</th><th>评价</th></tr></thead><tbody><tr><td>Refusion (IROS 2019)</td><td>剔除运动物体后的三维重建系统</td><td>使用残差找到动态部分的点，使用填充算法得到动态部分区域，剔除动态部分使用静态部分重建</td><td>分割结果总是比实际的要大、重建失败、重影</td></tr><tr><td>BAD Slam (CVPR 2019)</td><td>实时重建</td><td>基于Surfel的BA快速rgb-d BA</td><td></td></tr><tr><td>Gradient-SDF (CVPR 2022)</td><td>静态重建，提出了新的三维模型表示方法，同时结合了显式（BA)和隐式(SDF Tracking)的优点</td><td>voxel存储存储梯度，可以计算出voxel对应的表面上最近的点，非常容易做相机追踪和BA，提高重建模型的精度</td><td></td></tr><tr><td>HRBF-Fusion(Siggraph 2022)</td><td>静态高精度三维重建</td><td>提出了一种新的基于Surfel的模型表示方法-Hermit Radial Basis Function，用于改进局部细节。并提出了基于HRBF的曲率、法线、置信度和新的融合策略，构建了3D重建系统</td><td>尝试使用了新的模型表示方法，不太容易改进</td></tr><tr><td>DRG-SLAM（IROS 2022）</td><td>删除运动物体的稀疏建模</td><td>动态点不满足极线约束、使用网络分割和多视图一致性去掉动态orb特征点，使用点线面损失计算相机位姿</td><td>可以去掉分割网络不能识别的动态物体</td></tr><tr><td>ACEFusion（IROS 2022）</td><td>同时重建前景和背景，rgbd实时动态环境重建</td><td>利用深度学习加速单元加快实例分割，使用光流和实例分割的方法得到初始的动态区域，然后使用flood-fill得到最终的动态区域掩码，之后使用octree重建静态场景，使用sufelwarp重建动态区域</td><td>同时重建动态区域和静态区域</td></tr><tr><td>PSgraident-SDF (WACV 2023)</td><td>光度立体重建：多视角RGB-D高精度重建系统，需要重建几何细节和丰富的纹理</td><td>给定模型的BRDF、法线和光照模型的初始值，利用渲染后的结果和拍摄得到的RGB图像来优化模型的BRDF、符号距离值以及相机位姿和灯光</td><td></td></tr></tbody></table></div>
<hr>
<h2 id="深度学习">深度学习<a aria-hidden="true" class="anchor-heading icon-link" href="#深度学习"></a></h2>
<div class="table-responsive">
































































<table><thead><tr><th>论文</th><th>解决的问题</th><th>采用的方法</th><th>评价</th></tr></thead><tbody><tr><td>Deepdeform(2019)</td><td>非刚性重建</td><td>神经网络来找前后像素的对应关系</td><td></td></tr><tr><td>Neural non-rigid tracking (CVPR 2020)</td><td>非刚性重建</td><td>引入了稠密光流</td><td>比deepdeform更高效</td></tr><tr><td>4DComplete (ICCV 2021)</td><td>非刚性重建</td><td>预测不可见表面的运动</td><td>计算复杂，不能实时</td></tr><tr><td>iMAP (ICCV 2021)</td><td>实时静态重建</td><td>使用隐式场表示三维模型，利用隐式场Nerf得到的深度和颜色与输入的损失来更新相机的位姿和网络。并提出了相应的关键帧选择策略</td><td></td></tr><tr><td>OcclusionFusion（CVPR 2022)</td><td>非刚性重建,相机固定</td><td>用光流估计2D运动，从而得到可见部分的三维运动，图神经网络和LSTM综合考虑了历史运动、当前运动信息来预测当前帧运动和置信度。完成t+1帧重建</td><td>基于图节点重建系统无法解决拓扑该改变(POSEFusion或者Neural Deformation Graphs）、长期遮挡的部分和大的形变</td></tr><tr><td>Neural RGB-D Surface Reconstruction（CVPR 2022）</td><td>高精度静态重建</td><td>执行BundleFusion得到初始的相机位姿估计，使用不同的mlp分别计算sdf和color，然后神经渲染得到的深度图和颜色图。</td><td></td></tr><tr><td>BNV-Fusion (CVPR 2022)</td><td>高精度静态重建</td><td>使用神经网络编码每一个深度图，然后bi-level融合成全局模型，然后优化全局模型</td><td>速度比传统方法慢、深度图s测量噪声会造成几何不连续</td></tr><tr><td>Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera（NIPS 2022）</td><td>RGBD动态场景重建，相机固定</td><td>使用可逆双射函数来预测规范空间中点的形变之后的位置，然后结合拓扑感知网络可以得到mlp sdf，通过神经渲染函数，可以得到该点的颜色，通过在视线方向上做积分，可以得到该视角下渲染后的结果。</td><td>- 效果依旧不如orb-slam2，可能是BA，无全局ba，预测能力取决于coarse fine</td></tr><tr><td>vox-fusion(ISMAR 2022)</td><td>高精度静态重建</td><td>相比于imap，网络模型表示不一样，八叉树+体素编码+全连接。可以表示free space，关键帧选择策略选择新出现区域多的帧。</td><td></td></tr></tbody></table></div></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#论文整理" title="论文整理">论文整理</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#传统方法" title="传统方法">传统方法</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#深度学习" title="深度学习">深度学习</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"xg7qa72kkuq1elnh2ewtsqv","title":"Review","desc":"","updated":1692878050516,"created":1677676839702,"custom":{},"fname":"pub.doctor.review","type":"note","vault":{"fsPath":".","selfContained":true,"name":"notes"},"contentHash":"0822fd3e0428840cbb5746c53cb3dff6","links":[],"anchors":{"论文整理":{"type":"header","text":"论文整理","value":"论文整理","line":8,"column":0,"depth":1},"传统方法":{"type":"header","text":"传统方法","value":"传统方法","line":10,"column":0,"depth":2},"深度学习":{"type":"header","text":"深度学习","value":"深度学习","line":24,"column":0,"depth":2}},"children":[],"parent":"hhz8q9pblj7dmva0w1jijb8","data":{}},"body":"\u003ch1 id=\"review\"\u003eReview\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#review\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"论文整理\"\u003e论文整理\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#论文整理\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch2 id=\"传统方法\"\u003e传统方法\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#传统方法\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cdiv class=\"table-responsive\"\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e论文\u003c/th\u003e\u003cth\u003e解决的问题\u003c/th\u003e\u003cth\u003e采用的方法\u003c/th\u003e\u003cth\u003e评价\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eRefusion (IROS 2019)\u003c/td\u003e\u003ctd\u003e剔除运动物体后的三维重建系统\u003c/td\u003e\u003ctd\u003e使用残差找到动态部分的点，使用填充算法得到动态部分区域，剔除动态部分使用静态部分重建\u003c/td\u003e\u003ctd\u003e分割结果总是比实际的要大、重建失败、重影\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eBAD Slam (CVPR 2019)\u003c/td\u003e\u003ctd\u003e实时重建\u003c/td\u003e\u003ctd\u003e基于Surfel的BA快速rgb-d BA\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eGradient-SDF (CVPR 2022)\u003c/td\u003e\u003ctd\u003e静态重建，提出了新的三维模型表示方法，同时结合了显式（BA)和隐式(SDF Tracking)的优点\u003c/td\u003e\u003ctd\u003evoxel存储存储梯度，可以计算出voxel对应的表面上最近的点，非常容易做相机追踪和BA，提高重建模型的精度\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eHRBF-Fusion(Siggraph 2022)\u003c/td\u003e\u003ctd\u003e静态高精度三维重建\u003c/td\u003e\u003ctd\u003e提出了一种新的基于Surfel的模型表示方法-Hermit Radial Basis Function，用于改进局部细节。并提出了基于HRBF的曲率、法线、置信度和新的融合策略，构建了3D重建系统\u003c/td\u003e\u003ctd\u003e尝试使用了新的模型表示方法，不太容易改进\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eDRG-SLAM（IROS 2022）\u003c/td\u003e\u003ctd\u003e删除运动物体的稀疏建模\u003c/td\u003e\u003ctd\u003e动态点不满足极线约束、使用网络分割和多视图一致性去掉动态orb特征点，使用点线面损失计算相机位姿\u003c/td\u003e\u003ctd\u003e可以去掉分割网络不能识别的动态物体\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eACEFusion（IROS 2022）\u003c/td\u003e\u003ctd\u003e同时重建前景和背景，rgbd实时动态环境重建\u003c/td\u003e\u003ctd\u003e利用深度学习加速单元加快实例分割，使用光流和实例分割的方法得到初始的动态区域，然后使用flood-fill得到最终的动态区域掩码，之后使用octree重建静态场景，使用sufelwarp重建动态区域\u003c/td\u003e\u003ctd\u003e同时重建动态区域和静态区域\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003ePSgraident-SDF (WACV 2023)\u003c/td\u003e\u003ctd\u003e光度立体重建：多视角RGB-D高精度重建系统，需要重建几何细节和丰富的纹理\u003c/td\u003e\u003ctd\u003e给定模型的BRDF、法线和光照模型的初始值，利用渲染后的结果和拍摄得到的RGB图像来优化模型的BRDF、符号距离值以及相机位姿和灯光\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\n\u003chr\u003e\n\u003ch2 id=\"深度学习\"\u003e深度学习\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#深度学习\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cdiv class=\"table-responsive\"\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e论文\u003c/th\u003e\u003cth\u003e解决的问题\u003c/th\u003e\u003cth\u003e采用的方法\u003c/th\u003e\u003cth\u003e评价\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eDeepdeform(2019)\u003c/td\u003e\u003ctd\u003e非刚性重建\u003c/td\u003e\u003ctd\u003e神经网络来找前后像素的对应关系\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eNeural non-rigid tracking (CVPR 2020)\u003c/td\u003e\u003ctd\u003e非刚性重建\u003c/td\u003e\u003ctd\u003e引入了稠密光流\u003c/td\u003e\u003ctd\u003e比deepdeform更高效\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e4DComplete (ICCV 2021)\u003c/td\u003e\u003ctd\u003e非刚性重建\u003c/td\u003e\u003ctd\u003e预测不可见表面的运动\u003c/td\u003e\u003ctd\u003e计算复杂，不能实时\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eiMAP (ICCV 2021)\u003c/td\u003e\u003ctd\u003e实时静态重建\u003c/td\u003e\u003ctd\u003e使用隐式场表示三维模型，利用隐式场Nerf得到的深度和颜色与输入的损失来更新相机的位姿和网络。并提出了相应的关键帧选择策略\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eOcclusionFusion（CVPR 2022)\u003c/td\u003e\u003ctd\u003e非刚性重建,相机固定\u003c/td\u003e\u003ctd\u003e用光流估计2D运动，从而得到可见部分的三维运动，图神经网络和LSTM综合考虑了历史运动、当前运动信息来预测当前帧运动和置信度。完成t+1帧重建\u003c/td\u003e\u003ctd\u003e基于图节点重建系统无法解决拓扑该改变(POSEFusion或者Neural Deformation Graphs）、长期遮挡的部分和大的形变\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eNeural RGB-D Surface Reconstruction（CVPR 2022）\u003c/td\u003e\u003ctd\u003e高精度静态重建\u003c/td\u003e\u003ctd\u003e执行BundleFusion得到初始的相机位姿估计，使用不同的mlp分别计算sdf和color，然后神经渲染得到的深度图和颜色图。\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eBNV-Fusion (CVPR 2022)\u003c/td\u003e\u003ctd\u003e高精度静态重建\u003c/td\u003e\u003ctd\u003e使用神经网络编码每一个深度图，然后bi-level融合成全局模型，然后优化全局模型\u003c/td\u003e\u003ctd\u003e速度比传统方法慢、深度图s测量噪声会造成几何不连续\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eNeural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera（NIPS 2022）\u003c/td\u003e\u003ctd\u003eRGBD动态场景重建，相机固定\u003c/td\u003e\u003ctd\u003e使用可逆双射函数来预测规范空间中点的形变之后的位置，然后结合拓扑感知网络可以得到mlp sdf，通过神经渲染函数，可以得到该点的颜色，通过在视线方向上做积分，可以得到该视角下渲染后的结果。\u003c/td\u003e\u003ctd\u003e- 效果依旧不如orb-slam2，可能是BA，无全局ba，预测能力取决于coarse fine\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003evox-fusion(ISMAR 2022)\u003c/td\u003e\u003ctd\u003e高精度静态重建\u003c/td\u003e\u003ctd\u003e相比于imap，网络模型表示不一样，八叉树+体素编码+全连接。可以表示free space，关键帧选择策略选择新出现区域多的帧。\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e","noteIndex":{"fname":"pub","stub":true,"vault":{"fsPath":".","selfContained":true,"name":"notes"},"schemaStub":false,"type":"note","updated":1698572854809,"created":1698572854809,"id":"3w8ufj0ddqin6wp485hfdsg","desc":"","links":[],"anchors":{},"children":["9rgzbgkb65netbhrh58dxnz","hhz8q9pblj7dmva0w1jijb8"],"parent":null,"body":"","data":{},"title":"Pub","custom":{"nav_order":0,"permalink":"/"}},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"notes"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false},"preview":{"theme":"custom","enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["pub"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal Knowledge Space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"master","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"search","theme":"dark","usePrettyRefs":true,"assetsPrefix":"/pub-notes","siteUrl":"https://jinjiwu.github.io","siteFaviconPath":"favicon.ico","siteIndex":"pub"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"xg7qa72kkuq1elnh2ewtsqv"},"buildId":"DPk29Lz7PV3gbz_lK8oCu","assetPrefix":"/pub-notes","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>